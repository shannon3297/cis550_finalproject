{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wsj_add_columns.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMuDEPAbyg50qmHtPPAPi37"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AOdTC5_r5etx","executionInfo":{"status":"ok","timestamp":1637186207934,"user_tz":300,"elapsed":22757,"user":{"displayName":"Shannon Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7cfJY2k_K6RchWg4pHrTZcLbCPf2aGZgW66AfV-A=s64","userId":"15730052516052696232"}},"outputId":"c4a498cb-647a-412c-ae5f-fde7a5dc801c"},"source":["# imports\n","from google.colab import drive\n","import pandas as pd\n","\n","# mount to drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive not mounted, so nothing to flush and unmount.\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"9hIQxmN95iq0","executionInfo":{"status":"ok","timestamp":1637186207935,"user_tz":300,"elapsed":9,"user":{"displayName":"Shannon Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7cfJY2k_K6RchWg4pHrTZcLbCPf2aGZgW66AfV-A=s64","userId":"15730052516052696232"}}},"source":["path = '/content/drive/MyDrive/F21/CIS550/Final Project/WSJ Articles/'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"iU42Shup5jkD","executionInfo":{"status":"ok","timestamp":1637194638015,"user_tz":300,"elapsed":1474,"user":{"displayName":"Shannon Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7cfJY2k_K6RchWg4pHrTZcLbCPf2aGZgW66AfV-A=s64","userId":"15730052516052696232"}}},"source":["wsj = pd.read_csv(path + 'oct20-oct21.csv')\n","stocks_raw = pd.read_csv(path + 'russell1000stocks.csv')"],"execution_count":133,"outputs":[]},{"cell_type":"code","metadata":{"id":"d436pnEQLyjL","executionInfo":{"status":"ok","timestamp":1637197288394,"user_tz":300,"elapsed":2048,"user":{"displayName":"Shannon Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7cfJY2k_K6RchWg4pHrTZcLbCPf2aGZgW66AfV-A=s64","userId":"15730052516052696232"}}},"source":["# clean stocks csv\n","import re\n","\n","stocks = stocks_raw.copy()\n","companies_cleaned = stocks_raw['Description']\n","for idx, company in enumerate(companies_cleaned):\n","  cleaned = company.replace(',', '')\n","  cleaned = cleaned.replace('.', '')\n","  cleaned = cleaned.replace('com', '')\n","  cleaned = cleaned.replace('Inc', '')\n","  cleaned = cleaned.replace('Inc', '')\n","  cleaned = cleaned.replace('Com', '')\n","  cleaned = cleaned.replace('Corp', '')\n","  cleaned = re.sub(\"Class [A-Za-z]\", \"\", cleaned)\n","  # 'News' is too common word and ticker 'A' too common (ticker/company name detection is not reliable with these companies)\n","  if cleaned == 'News':\n","    stocks.drop([idx], inplace=True)\n","  elif cleaned == 'Agilent Technologies':\n","    stocks.drop([idx], inplace=True)\n","  else:\n","    companies_cleaned[idx] = cleaned.strip()\n","stocks['Description'] = companies_cleaned\n","stocks.to_csv(path + 'russell1000stocks_cleaned.csv', index = False)"],"execution_count":202,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjZlbWh16_xE","executionInfo":{"status":"ok","timestamp":1637197542491,"user_tz":300,"elapsed":123,"user":{"displayName":"Shannon Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7cfJY2k_K6RchWg4pHrTZcLbCPf2aGZgW66AfV-A=s64","userId":"15730052516052696232"}}},"source":["# create stock ticker/company name to sector mappings\n","raw_tickers = stocks['Symbol'].values\n","raw_companies = stocks['Description'].values\n","tickers = ' '.join([str(elem) for elem in stocks['Symbol']]) # ignore Agilent stock ticker\n","companies = ' '.join([str(elem) for elem in stocks['Description']])\n","tickers_to_company = dict(zip(stocks['Symbol'], stocks['Description']))\n","tickers_to_sector = dict(zip(stocks['Symbol'], stocks['GICS Sector']))\n","company_to_sector = dict(zip(stocks['Description'], stocks['GICS Sector']))\n","# print(tickers)\n","# print(companies)\n","# print(tickers_to_sector)"],"execution_count":207,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNCwuvmb5yam","executionInfo":{"status":"ok","timestamp":1637197586293,"user_tz":300,"elapsed":42420,"user":{"displayName":"Shannon Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7cfJY2k_K6RchWg4pHrTZcLbCPf2aGZgW66AfV-A=s64","userId":"15730052516052696232"}},"outputId":"ef3f8834-6b0f-42d2-d44e-7339d8767c44"},"source":["# ticker\n","num_articles = len(wsj)\n","print(\"Beginning to add columns to\", num_articles, \"articles\")\n","title_ticker_mentions = [None] * num_articles\n","subtitle_ticker_mentions = [None] * num_articles\n","content_ticker_mentions = [None] * num_articles\n","# company names\n","title_company_mentions = [None] * num_articles\n","subtitle_company_mentions = [None] * num_articles\n","content_company_mentions = [None] * num_articles\n","# sector\n","title_sector_mentions = [None] * num_articles\n","subtitle_sector_mentions = [None] * num_articles\n","content_sector_mentions = [None] * num_articles\n","i = 0\n","for index, row in wsj.iterrows():\n","    currTitle = row['Title']\n","    currSubtitle = row['Subtitle']\n","    currContent = row['Content']\n","    # TICKERS\n","    # title\n","    title_ticker = set(tickers.split()).intersection(set(currTitle.split()))\n","    cleaned_title_ticker = []\n","    if type(title_ticker) == 'set' and title_ticker is not None:\n","      for tick in title_ticker:\n","        if tick in raw_tickers:\n","          cleaned_title_ticker.append(tick)\n","    if len(cleaned_title_ticker) != 0:\n","      title_ticker_mentions[index] = cleaned_title_ticker\n","    # subtitle\n","    subtitle_ticker = set(tickers.split()).intersection(set(currSubtitle.split()))\n","    cleaned_subtitle_ticker = []\n","    if type(subtitle_ticker) == 'set' and subtitle_ticker is not None:\n","      for tick in subtitle_ticker:\n","        if tick in raw_tickers:\n","          cleaned_subtitle_ticker.append(tick)\n","    if len(cleaned_subtitle_ticker) != 0:\n","      subtitle_ticker_mentions[index] = cleaned_subtitle_ticker\n","    # content\n","    content_ticker = set(tickers.split()).intersection(set(currContent.split()))\n","    cleaned_content_ticker = []\n","    if type(content_ticker) == 'set' and content_ticker is not None:\n","      for tick in content_ticker:\n","        if tick in raw_tickers:\n","           cleaned_contentticker.append(tick)\n","    if len(cleaned_content_ticker) != 0:\n","      content_ticker_mentions[index] = cleaned_content_ticker\n","    # COMPANY NAMES\n","    # title\n","    title_company = set(companies.split()).intersection(set(currTitle.split()))\n","    # print(\"title_company\", title_company)\n","    cleaned_title_company = []\n","    for title in title_company:\n","      if title in raw_companies:\n","        cleaned_title_company.append(title)\n","    # print(\"pre\", cleaned_title_company)\n","    if type(title_ticker) == 'set' and title_ticker is not None:\n","      for tick in cleaned_title_ticker:\n","        cleaned_title_company.add(tickers_to_company[tick])\n","    # print(\"post\", cleaned_title_company)\n","    if len(cleaned_title_company) != 0:\n","      title_company_mentions[index] = cleaned_title_company\n","    # subtitle\n","    subtitle_company = set(companies.split()).intersection(set(currSubtitle.split()))\n","    cleaned_subtitle_company = []\n","    for title in subtitle_company:\n","      if title in raw_companies:\n","        cleaned_subtitle_company.append(title)\n","    if type(subtitle_ticker) == 'set' and subtitle_ticker is not None:\n","      for tick in cleaned_subtitle_ticker:\n","        cleaned_subtitle_company.add(tickers_to_company[tick])\n","    if len(cleaned_subtitle_company) != 0:\n","      subtitle_company_mentions[index] = cleaned_subtitle_company\n","    # content\n","    content_company = set(companies.split()).intersection(set(currContent.split()))\n","    cleaned_content_company = []\n","    for title in content_company:\n","      if title in raw_companies:\n","        cleaned_content_company.append(title)\n","    if type(content_ticker) == 'set' and content_ticker is not None:\n","      for tick in cleaned_content_ticker:\n","        cleaned_content_company.add(tickers_to_company[tick])\n","    if len(cleaned_content_company) != 0:\n","      content_company_mentions[index] = cleaned_content_company\n","    # SECTORS\n","    # title\n","    title_sector = []\n","    title_sector.extend([company_to_sector[x] for x in cleaned_title_company])\n","    title_sector.extend([tickers_to_sector[x] for x in cleaned_title_ticker])\n","    if len(title_sector) != 0:\n","      title_sector_mentions[index] = title_sector\n","    # subtitle\n","    subtitle_sector = []\n","    subtitle_sector.extend([company_to_sector[x] for x in cleaned_subtitle_company])\n","    subtitle_sector.extend([tickers_to_sector[x] for x in cleaned_subtitle_ticker])\n","    if len(subtitle_sector) != 0:\n","      subtitle_sector_mentions[index] = subtitle_sector\n","    # content\n","    content_sector = []\n","    content_sector.extend([company_to_sector[x] for x in cleaned_content_company])\n","    content_sector.extend([tickers_to_sector[x] for x in cleaned_content_ticker])\n","    if len(subtitle_sector) != 0:\n","      content_sector_mentions[index] = content_sector\n","    i += 1\n","    # if i == 10:\n","    #   break\n","    if i % 500 == 0:\n","      print(\"Finished adding columns to\", i, \"records\")\n","print(\"Finished all records\")\n","wsj['Tickers mentioned in title'] = title_ticker_mentions\n","wsj['Tickers mentioned in subtitle'] = subtitle_ticker_mentions\n","wsj['Tickers mentioned in content'] = content_ticker_mentions\n","wsj['Company names mentioned in title'] = title_company_mentions\n","wsj['Company names mentioned in subtitle'] = subtitle_company_mentions\n","wsj['Company names mentioned in content'] = content_company_mentions\n","wsj['Sectors mentioned in title'] = title_sector_mentions\n","wsj['Sectors mentioned in subtitle'] = subtitle_sector_mentions\n","wsj['Sectors mentioned in content'] = content_sector_mentions\n","wsj.to_csv(path+'oct20-oct21_revamped.csv', index=False)"],"execution_count":208,"outputs":[{"output_type":"stream","name":"stdout","text":["Beginning to add columns to 15414 articles\n","Finished adding columns to 500 records\n","Finished adding columns to 1000 records\n","Finished adding columns to 1500 records\n","Finished adding columns to 2000 records\n","Finished adding columns to 2500 records\n","Finished adding columns to 3000 records\n","Finished adding columns to 3500 records\n","Finished adding columns to 4000 records\n","Finished adding columns to 4500 records\n","Finished adding columns to 5000 records\n","Finished adding columns to 5500 records\n","Finished adding columns to 6000 records\n","Finished adding columns to 6500 records\n","Finished adding columns to 7000 records\n","Finished adding columns to 7500 records\n","Finished adding columns to 8000 records\n","Finished adding columns to 8500 records\n","Finished adding columns to 9000 records\n","Finished adding columns to 9500 records\n","Finished adding columns to 10000 records\n","Finished adding columns to 10500 records\n","Finished adding columns to 11000 records\n","Finished adding columns to 11500 records\n","Finished adding columns to 12000 records\n","Finished adding columns to 12500 records\n","Finished adding columns to 13000 records\n","Finished adding columns to 13500 records\n","Finished adding columns to 14000 records\n","Finished adding columns to 14500 records\n","Finished adding columns to 15000 records\n","Finished all records\n"]}]}]}